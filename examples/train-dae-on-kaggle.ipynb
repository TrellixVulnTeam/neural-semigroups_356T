{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this if you've just uploaded this notebook to Google Colaboratory\n",
    "# better use a GPU runtime (TPU ones are not supported by the package yet)\n",
    "!pip install neural-semigroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a Cayley database, you can build a machine learning model for such a task:\n",
    "\n",
    "Given a partially filled Cayley table of a semigroup, restore the full one.\n",
    "\n",
    "It should be mentioned that a partially filled table sometimes can be filled in several ways to a full associative table. We will consider all such solutions as equally valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `neural-semigroups` package we use `torch` for building deep learning models.\n",
    "\n",
    "First of all, we need to get some training and validation data.\n",
    "In this example, we take semigroups of 5 items, and hold 100 Cayley tables (each representing a different class of equivalent semigrous) as our training data, and another 100 tables as validation.\n",
    "This is a rough 10/90 split of all tables of 5 elements available (there are 1160 of them up to equivalence).\n",
    "\n",
    "Here we construct `DataLoaders` for `torch` which will feed a training pipeline with 512 tables at a time.\n",
    "This number (batch size) can be changed for fine-tuning the model's quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_semigroups.training_helpers import get_loaders\n",
    "\n",
    "cardinality = 5\n",
    "dropout_rate = 0.5\n",
    "data_loaders = get_loaders(\n",
    "    cardinality=cardinality,\n",
    "    batch_size=512,\n",
    "    train_size=100,\n",
    "    validation_size=100,\n",
    "    dropout_rate=dropout_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for a training set we:\n",
    "* take 100 representatives of different equivalence classes\n",
    "* augment data by adding all equivalent tables\n",
    "* as a result, we will train on 16100 tables from 100 classes of equivalence\n",
    "\n",
    "For validation (for early stopping during training) we simply use 100 tables from different classes.\n",
    "\n",
    "From each of the rest 960 equivalence classes one table goes into a test dataset on wich the trained model is finally evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model each input Cayley table as a three index tensor $a_{ijk}$ such that\n",
    "\n",
    "$a_{ijk}=P\\left\\{e_ie_j=e_k\\right\\}$\n",
    "\n",
    "where $e_i$ are elements of a semigroup.\n",
    "\n",
    "In our training data all $a_{ijk}$ are either zeros or ones, so probability distributions involved are degenerate.\n",
    "\n",
    "When we need to hide a cell with indices $i,j$ from an original Cayley table we set\n",
    "\n",
    "$a_{ijk}=\\dfrac1n$\n",
    "\n",
    "where $n$ is the semigroup's cardinality. Thus we set a probability distribution of the multiplication result $e_ie_j$ to discrete uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a simple denoising autoencoder as an architecture for our neural network. It simply gets an input tensor of zeros and ones, hides 50% of input cells in a manner described earlier, and applies a linear transformation into a higher dimension ($n^5$ which is contrary to a common idea of autoencoders) with a simple `ReLU` non-linearity. Then another linear transformation to the same dimension with `ReLU` is applied, and then the last one to return back to the original $n^3$ dimension. We also apply batch normalization here. See the package code for the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_semigroups.denoising_autoencoder import MagmaDAE\n",
    "from neural_semigroups.constants import CURRENT_DEVICE\n",
    "\n",
    "dae = MagmaDAE(\n",
    "    cardinality=cardinality,\n",
    "    hidden_dims=2 * [cardinality ** 5],\n",
    "    dropout_rate=dropout_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, our model has ca 20 million  parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in dae.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the input and output are some probability distributions, we can employ a KL divergence as a measure of their similarity and a loss function for our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import kl_div\n",
    "from torch import Tensor\n",
    "import torch\n",
    "\n",
    "def loss(prediction: Tensor, target: Tensor) -> Tensor:\n",
    "    return kl_div(torch.log(prediction), target, reduction=\"batchmean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells we will run `tensorboard` to show training/validation curves during training process.\n",
    "\n",
    "Metrics on the test dataset are depicted by points (one for the whole dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8b61830f996c082\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8b61830f996c082\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_semigroups.training_helpers import learning_pipeline\n",
    "from ignite.metrics.loss import Loss\n",
    "from neural_semigroups.training_helpers import associative_ratio, guessed_ratio\n",
    "\n",
    "dae = MagmaDAE(\n",
    "    cardinality=cardinality,\n",
    "    hidden_dims=2 * [cardinality ** 5],\n",
    "    dropout_rate=dropout_rate\n",
    ")\n",
    "params = {\"learning_rate\": 0.0001, \"epochs\": 1000}\n",
    "metrics = {\n",
    "    \"loss\": Loss(loss),\n",
    "    \"associative_ratio\": Loss(associative_ratio),\n",
    "    \"guessed_ratio\": Loss(guessed_ratio)\n",
    "}\n",
    "learning_pipeline(params, dae, loss, metrics, data_loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can observe that although we ask the network to reproduce the input exactly, it's not very successful at this task (final guess ratio is about 10%). But the network surely learns something about associativity (it generates about 60% of associative tables on the test set). That suggests one can concetrate not on actually guessing the undistorted input but on generating something associative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible choice of a loss function to minimize is a special [associator loss](https://neural-semigroups.readthedocs.io/en/latest/package-documentation.html#associator-loss). When the network produces an output which differs from the input but is associative, the classical DAE loss punishes it, but this one does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_semigroups.associator_loss import AssociatorLoss\n",
    "\n",
    "def loss(prediction: Tensor, target: Tensor) -> Tensor:\n",
    "    return AssociatorLoss()(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dae = MagmaDAE(\n",
    "    cardinality=cardinality,\n",
    "    hidden_dims=2 * [cardinality ** 5],\n",
    "    dropout_rate=dropout_rate\n",
    ")\n",
    "learning_pipeline(params, dae, loss, metrics, data_loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the associator loss leads to better results. Moreover, it starts guessing the original input more often which is somewhat unexpected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To observe, how its success is dependend on the choice of 100 training tables, we repeat the whole pipeline 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    torch.manual_seed(i)\n",
    "    data_loaders = get_loaders(\n",
    "        cardinality=cardinality,\n",
    "        batch_size=512,\n",
    "        train_size=100,\n",
    "        validation_size=100,\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    dae = MagmaDAE(\n",
    "        cardinality=cardinality,\n",
    "        hidden_dims=2 * [cardinality ** 5],\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    learning_pipeline(params, dae, loss, metrics, data_loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model generalizes well (it was trained only on one tenth of all equivalence classes), although the overall quality depends on the data selected for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how it works on several examples of puzzles. Let's take one of the real tables from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_semigroups import CayleyDatabase\n",
    "\n",
    "cayley_db = CayleyDatabase(cardinality)\n",
    "cayley_db.model = dae\n",
    "cayley_db.database[1100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can fill it with `-1` in some cells, creating a puzzle and giving it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess, proba = cayley_db.fill_in_with_model([\n",
    "  [-1, 0, 0, 0, -1],\n",
    "  [0, -1, 1, 1, -1],\n",
    "  [0, 1, -1, 1, -1],\n",
    "  [0, 1, 1, -1, -1],\n",
    "  [0, 1, 1, 1, -1]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model found not the same table as the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it's still a possible completion since it's associative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_semigroups import Magma\n",
    "\n",
    "Magma(guess).is_associative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model returns also it's probabilities of guess. They can be examined in cases when the model err."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

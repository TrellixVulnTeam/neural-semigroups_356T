{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "   Copyright 2019-2020 Boris Shminke\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a Cayley database, you can build a machine learning model for such a task:\n",
    "\n",
    "Given a partially filled Cayley table of a semigroup, restore the full one.\n",
    "\n",
    "It should be mentioned that a partially filled table sometimes can be filled in several ways to a full associative table. We will consider all such solutions as equally valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `neural-semigroups` package we use `torch` for building deep learning models.\n",
    "\n",
    "First of all, we need to get some training and validation data.\n",
    "In this example, we take semigroups of 7 items, and hold 100 Cayley tables (each representing a different class of equivalent semigrous) as our training data, and another 100 tables as validation.\n",
    "This is a really small fraction of all tables of 7 elements available (there are 836021 of them up to equivalence).\n",
    "\n",
    "Here we construct `DataLoaders` for `torch` which will feed a training pipeline with 48 tables at a time.\n",
    "This number (batch size) can be changed for fine-tuning the model's quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating train cubes: 100%|██████████| 100/100 [00:00<00:00, 28633.97it/s]\n",
      "generating validation cubes: 100%|██████████| 100/100 [00:00<00:00, 19415.38it/s]\n",
      "generating test cubes: 100%|██████████| 835821/835821 [00:26<00:00, 32105.40it/s]\n"
     ]
    }
   ],
   "source": [
    "from neural_semigroups.training_helpers import get_loaders\n",
    "\n",
    "cardinality = 7\n",
    "data_loaders = get_loaders(\n",
    "    cardinality=cardinality,\n",
    "    batch_size=48,\n",
    "    train_size=100,\n",
    "    validation_size=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model each input Cayley table as a three index tensor $a_{ijk}$ such that\n",
    "\n",
    "$a_{ijk}=P\\left\\{e_ie_j=e_k\\right\\}$\n",
    "\n",
    "where $e_i$ are elements of a semigroup.\n",
    "\n",
    "In our training data all $a_{ijk}$ are either zeros or ones, so probability distributions involved are degenerate.\n",
    "\n",
    "When we need to hide a cell with indices $i,j$ from an original Cayley table we set\n",
    "\n",
    "$a_{ijk}=\\dfrac1n$\n",
    "\n",
    "where $n$ is the semigroup's cardinality. Thus we set a probability distribution of the multiplication result $e_ie_j$ to discrete uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a simple denoising autoencoder as an architecture for our neural network. It simply gets an input tensor of zeros and ones, hide 50% of input cells in a manner described earlier, and applies a linear transformation into a lower dimension ($n^2$ but these also can be tuned) with a simple `RuLU` non-linearity. Then another linear transformation with `ReLU` is applied to return back to the original $n^3$ dimension. We also apply batch norm here. See the package code for the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_semigroups import MagmaDAE\n",
    "\n",
    "dae = MagmaDAE(\n",
    "    cardinality=cardinality,\n",
    "    hidden_dims=[\n",
    "        cardinality ** 2\n",
    "    ],\n",
    "    corruption_rate=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MagmaDAE(\n",
       "  (decoder_layers): Sequential(\n",
       "    (linear10): Linear(in_features=49, out_features=343, bias=True)\n",
       "    (relu10): ReLU()\n",
       "    (bn10): BatchNorm1d(343, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (encoder_layers): Sequential(\n",
       "    (linear00): Linear(in_features=343, out_features=49, bias=True)\n",
       "    (relu00): ReLU()\n",
       "    (bn00): BatchNorm1d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training process we try to minimize the Kullback-Leibler distance between two probabilistic cubes (the input and output ones of the DAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.functional import kl_div\n",
    "\n",
    "def loss(prediction: Tensor, target: Tensor) -> Tensor:\n",
    "    return kl_div(torch.log(prediction), target, reduction=\"batchmean\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `pytorch-ignite` to write less boilerplate code for a training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import create_supervised_evaluator\n",
    "from ignite.metrics.loss import Loss\n",
    "\n",
    "evaluator = create_supervised_evaluator(\n",
    "    dae,\n",
    "    metrics={\"loss\": Loss(loss)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to run a pipeline! Here you can tune the learning schedule for better results.\n",
    "\n",
    "You can construct your own pipeline if you don't want to import one provided by the package.\n",
    "\n",
    "In the next three cells we will run `tensorboard` to show training/validation curves during training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-11d902688fcf0817\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-11d902688fcf0817\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6007;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.4 s, sys: 1.92 s, total: 1min\n",
      "Wall time: 40.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from neural_semigroups.training_helpers import learning_pipeline\n",
    "\n",
    "params = {\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"epochs\": 1000,\n",
    "    \"cardinality\": cardinality\n",
    "}\n",
    "learning_pipeline(params, dae, evaluator, loss, data_loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the report of results. It seems to be quite impressive. For it we got 1000 of random Cayley tables (for different equivalent classes as always) and constructed 'puzzles' from it.\n",
    "\n",
    "Level of difficulty for a puzzle is a number of hidden cells. A puzzle is considered to be solved if the model returns a full associative table.\n",
    "\n",
    "We see that the model generalizes well (it was trained only on 100 tables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating and solving puzzles: 100%|██████████| 1000/1000 [00:57<00:00, 17.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>puzzles</th>\n",
       "      <th>solved</th>\n",
       "      <th>(%)</th>\n",
       "      <th>hidden cells</th>\n",
       "      <th>guessed</th>\n",
       "      <th>in %</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>892</td>\n",
       "      <td>89</td>\n",
       "      <td>1000</td>\n",
       "      <td>892</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>840</td>\n",
       "      <td>84</td>\n",
       "      <td>2000</td>\n",
       "      <td>1804</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>810</td>\n",
       "      <td>81</td>\n",
       "      <td>3000</td>\n",
       "      <td>2704</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>778</td>\n",
       "      <td>77</td>\n",
       "      <td>4000</td>\n",
       "      <td>3597</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1000</td>\n",
       "      <td>759</td>\n",
       "      <td>75</td>\n",
       "      <td>5000</td>\n",
       "      <td>4507</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1000</td>\n",
       "      <td>750</td>\n",
       "      <td>75</td>\n",
       "      <td>6000</td>\n",
       "      <td>5409</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1000</td>\n",
       "      <td>740</td>\n",
       "      <td>74</td>\n",
       "      <td>7000</td>\n",
       "      <td>6277</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1000</td>\n",
       "      <td>747</td>\n",
       "      <td>74</td>\n",
       "      <td>8000</td>\n",
       "      <td>7189</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1000</td>\n",
       "      <td>736</td>\n",
       "      <td>73</td>\n",
       "      <td>9000</td>\n",
       "      <td>8033</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1000</td>\n",
       "      <td>733</td>\n",
       "      <td>73</td>\n",
       "      <td>10000</td>\n",
       "      <td>9007</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1000</td>\n",
       "      <td>722</td>\n",
       "      <td>72</td>\n",
       "      <td>11000</td>\n",
       "      <td>9889</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1000</td>\n",
       "      <td>731</td>\n",
       "      <td>73</td>\n",
       "      <td>12000</td>\n",
       "      <td>10769</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1000</td>\n",
       "      <td>727</td>\n",
       "      <td>72</td>\n",
       "      <td>13000</td>\n",
       "      <td>11609</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1000</td>\n",
       "      <td>727</td>\n",
       "      <td>72</td>\n",
       "      <td>14000</td>\n",
       "      <td>12540</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1000</td>\n",
       "      <td>723</td>\n",
       "      <td>72</td>\n",
       "      <td>15000</td>\n",
       "      <td>13359</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1000</td>\n",
       "      <td>723</td>\n",
       "      <td>72</td>\n",
       "      <td>16000</td>\n",
       "      <td>14280</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1000</td>\n",
       "      <td>724</td>\n",
       "      <td>72</td>\n",
       "      <td>17000</td>\n",
       "      <td>15191</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1000</td>\n",
       "      <td>715</td>\n",
       "      <td>71</td>\n",
       "      <td>18000</td>\n",
       "      <td>16076</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1000</td>\n",
       "      <td>716</td>\n",
       "      <td>71</td>\n",
       "      <td>19000</td>\n",
       "      <td>16838</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1000</td>\n",
       "      <td>727</td>\n",
       "      <td>72</td>\n",
       "      <td>20000</td>\n",
       "      <td>17903</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1000</td>\n",
       "      <td>711</td>\n",
       "      <td>71</td>\n",
       "      <td>21000</td>\n",
       "      <td>18743</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1000</td>\n",
       "      <td>728</td>\n",
       "      <td>72</td>\n",
       "      <td>22000</td>\n",
       "      <td>19593</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1000</td>\n",
       "      <td>717</td>\n",
       "      <td>71</td>\n",
       "      <td>23000</td>\n",
       "      <td>20490</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1000</td>\n",
       "      <td>716</td>\n",
       "      <td>71</td>\n",
       "      <td>24000</td>\n",
       "      <td>21374</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       puzzles  solved  (%)  hidden cells  guessed  in %\n",
       "level                                                   \n",
       "1         1000     892   89          1000      892    89\n",
       "2         1000     840   84          2000     1804    90\n",
       "3         1000     810   81          3000     2704    90\n",
       "4         1000     778   77          4000     3597    89\n",
       "5         1000     759   75          5000     4507    90\n",
       "6         1000     750   75          6000     5409    90\n",
       "7         1000     740   74          7000     6277    89\n",
       "8         1000     747   74          8000     7189    89\n",
       "9         1000     736   73          9000     8033    89\n",
       "10        1000     733   73         10000     9007    90\n",
       "11        1000     722   72         11000     9889    89\n",
       "12        1000     731   73         12000    10769    89\n",
       "13        1000     727   72         13000    11609    89\n",
       "14        1000     727   72         14000    12540    89\n",
       "15        1000     723   72         15000    13359    89\n",
       "16        1000     723   72         16000    14280    89\n",
       "17        1000     724   72         17000    15191    89\n",
       "18        1000     715   71         18000    16076    89\n",
       "19        1000     716   71         19000    16838    88\n",
       "20        1000     727   72         20000    17903    89\n",
       "21        1000     711   71         21000    18743    89\n",
       "22        1000     728   72         22000    19593    89\n",
       "23        1000     717   71         23000    20490    89\n",
       "24        1000     716   71         24000    21374    89"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neural_semigroups.utils import print_report\n",
    "from neural_semigroups import CayleyDatabase\n",
    "\n",
    "cardinality = 7\n",
    "cayley_db = CayleyDatabase(cardinality)\n",
    "cayley_db.load_model(f\"semigroups.{cardinality}.model\")\n",
    "print_report(cayley_db.testing_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "name": "train_a_model.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "   Copyright 2019-2020 Boris Shminke\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a Cayley database, you can build a machine learning model for such a task:\n",
    "\n",
    "Given a partially filled Cayley table of a semigroup, restore the full one.\n",
    "\n",
    "It should be mentioned that a partially filled table sometimes can be filled in several ways to a full associative table. We will consider all such solutions as equally valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `neural-semigroups` package we use `torch` for building deep learning models.\n",
    "\n",
    "First of all, we need to get some training and validation data.\n",
    "In this example, we take semigroups of 4 items, and hold 63 Cayley tables (each representing a different class of equivalent semigrous) as our training data, and another 62 tables as validation.\n",
    "This is a rough 50/50 spplit of all tables of 4 elements available (there are 126 of them up to equivalence).\n",
    "\n",
    "Here we construct `DataLoaders` for `torch` which will feed a training pipeline with 512 tables at a time.\n",
    "This number (batch size) can be changed for fine-tuning the model's quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "augmenting by equivalent tables: 100%|██████████| 63/63 [00:00<00:00, 715.61it/s]\n",
      "generating train cubes: 100%|██████████| 1754/1754 [00:00<00:00, 60926.46it/s]\n",
      "generating validation cubes: 100%|██████████| 62/62 [00:00<00:00, 50790.40it/s]\n",
      "generating test cubes: 100%|██████████| 1/1 [00:00<00:00, 3463.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from neural_semigroups.training_helpers import get_loaders\n",
    "\n",
    "cardinality = 4\n",
    "data_loaders = get_loaders(\n",
    "    cardinality=cardinality,\n",
    "    batch_size=512,\n",
    "    train_size=63,\n",
    "    validation_size=62\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for a training set we:\n",
    "* take 63 representatives of different equivalence classes\n",
    "* augment data by adding all equivalent tables\n",
    "* as a result, we will train on 1754 tables from 63 classes of equivalence\n",
    "\n",
    "For validation we simply use 62 tables from different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model each input Cayley table as a three index tensor $a_{ijk}$ such that\n",
    "\n",
    "$a_{ijk}=P\\left\\{e_ie_j=e_k\\right\\}$\n",
    "\n",
    "where $e_i$ are elements of a semigroup.\n",
    "\n",
    "In our training data all $a_{ijk}$ are either zeros or ones, so probability distributions involved are degenerate.\n",
    "\n",
    "When we need to hide a cell with indices $i,j$ from an original Cayley table we set\n",
    "\n",
    "$a_{ijk}=\\dfrac1n$\n",
    "\n",
    "where $n$ is the semigroup's cardinality. Thus we set a probability distribution of the multiplication result $e_ie_j$ to discrete uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a simple denoising autoencoder as an architecture for our neural network. It simply gets an input tensor of zeros and ones, hide 50% of input cells in a manner described earlier, and applies a linear transformation into a higher dimension ($n^8$ which is contrary to a common idea of autoencoders) with a simple `RuLU` non-linearity. Then another linear transformation with `ReLU` is applied to return back to the original $n^3$ dimension. We also apply batch normalization here. See the package code for the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_semigroups import MagmaDAE\n",
    "from neural_semigroups.constants import CURRENT_DEVICE\n",
    "\n",
    "dae = MagmaDAE(\n",
    "    cardinality=cardinality,\n",
    "    hidden_dims=[\n",
    "        cardinality ** 8\n",
    "    ],\n",
    "    corruption_rate=0.5\n",
    ").to(CURRENT_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MagmaDAE(\n",
       "  (decoder_layers): Sequential(\n",
       "    (linear10): Linear(in_features=65536, out_features=64, bias=True)\n",
       "    (relu10): ReLU()\n",
       "    (bn10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (encoder_layers): Sequential(\n",
       "    (linear00): Linear(in_features=64, out_features=65536, bias=True)\n",
       "    (relu00): ReLU()\n",
       "    (bn00): BatchNorm1d(65536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training process we try to minimize a special [associator loss](https://neural-semigroups.readthedocs.io/en/latest/package-documentation.html#associator-loss) on the output of the DAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from neural_semigroups import AssociatorLoss\n",
    "\n",
    "def loss(prediction: Tensor, target: Tensor) -> Tensor:\n",
    "    return AssociatorLoss()(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `pytorch-ignite` to write less boilerplate code for a training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import create_supervised_evaluator\n",
    "from ignite.metrics.loss import Loss\n",
    "\n",
    "evaluator = create_supervised_evaluator(\n",
    "    dae,\n",
    "    metrics={\"loss\": Loss(loss)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to run a pipeline! Here you can tune the learning schedule for better results.\n",
    "\n",
    "You can construct your own pipeline if you don't want to import one provided by the package.\n",
    "\n",
    "In the next three cells we will run `tensorboard` to show training/validation curves during training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 3152), started 2:07:13 ago. (Use '!kill 3152' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3a3108ff58739728\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3a3108ff58739728\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 24min 30s, sys: 3min 49s, total: 1h 28min 19s\n",
      "Wall time: 27min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from neural_semigroups.training_helpers import learning_pipeline\n",
    "\n",
    "params = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 1000,\n",
    "    \"cardinality\": cardinality\n",
    "}\n",
    "learning_pipeline(params, dae, evaluator, loss, data_loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the report of results. It seems to be quite impressive. For it we got 126 of all Cayley tables from 4 elements (for different equivalent classes as always) and constructed 'puzzles' from it.\n",
    "\n",
    "Level of difficulty for a puzzle is a number of hidden cells. A puzzle is considered to be solved if the model returns a full associative table.\n",
    "\n",
    "We see that the model generalizes well (it was trained only on a half of equivalence classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating and solving puzzles: 100%|██████████| 126/126 [00:06<00:00, 18.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>puzzles</th>\n",
       "      <th>solved</th>\n",
       "      <th>(%)</th>\n",
       "      <th>hidden cells</th>\n",
       "      <th>guessed</th>\n",
       "      <th>in %</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126</td>\n",
       "      <td>125</td>\n",
       "      <td>99</td>\n",
       "      <td>126</td>\n",
       "      <td>125</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>126</td>\n",
       "      <td>120</td>\n",
       "      <td>95</td>\n",
       "      <td>252</td>\n",
       "      <td>246</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>126</td>\n",
       "      <td>115</td>\n",
       "      <td>91</td>\n",
       "      <td>378</td>\n",
       "      <td>364</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126</td>\n",
       "      <td>109</td>\n",
       "      <td>86</td>\n",
       "      <td>504</td>\n",
       "      <td>482</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>126</td>\n",
       "      <td>113</td>\n",
       "      <td>89</td>\n",
       "      <td>630</td>\n",
       "      <td>608</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>126</td>\n",
       "      <td>101</td>\n",
       "      <td>80</td>\n",
       "      <td>756</td>\n",
       "      <td>708</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>126</td>\n",
       "      <td>104</td>\n",
       "      <td>82</td>\n",
       "      <td>882</td>\n",
       "      <td>841</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>126</td>\n",
       "      <td>104</td>\n",
       "      <td>82</td>\n",
       "      <td>1008</td>\n",
       "      <td>948</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       puzzles  solved  (%)  hidden cells  guessed  in %\n",
       "level                                                   \n",
       "1          126     125   99           126      125    99\n",
       "2          126     120   95           252      246    97\n",
       "3          126     115   91           378      364    96\n",
       "4          126     109   86           504      482    95\n",
       "5          126     113   89           630      608    96\n",
       "6          126     101   80           756      708    93\n",
       "7          126     104   82           882      841    95\n",
       "8          126     104   82          1008      948    94"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neural_semigroups.utils import print_report\n",
    "from neural_semigroups import CayleyDatabase\n",
    "\n",
    "cayley_db = CayleyDatabase(cardinality)\n",
    "cayley_db.load_model(f\"semigroups.{cardinality}.model\")\n",
    "print_report(cayley_db.testing_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how it works on several example puzzles. Let's take one of the real tables from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 2, 2],\n",
       "       [0, 0, 3, 3]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cayley_db.database[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can fill it with `-1` in some cells, creating a puzzle and giving it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess, proba = cayley_db.fill_in_with_model([\n",
    "    [0, 0, 0, 0],\n",
    "    [0, -1, -1, 0],\n",
    "    [0, -1, -1, 2],\n",
    "    [0, 0, 3, -1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model found not the same table as the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 2, 2],\n",
       "       [0, 0, 3, 3]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it's still a possible completion since it's associative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neural_semigroups import Magma\n",
    "\n",
    "Magma(guess).is_associative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model returns also it's probabilities of guess. They can be examined in cases when the model err."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[9.9999702e-01, 1.0000000e-06, 1.0000000e-06, 1.0000000e-06],\n",
       "        [9.9999702e-01, 1.0000000e-06, 1.0000000e-06, 1.0000000e-06],\n",
       "        [9.9999702e-01, 1.0000000e-06, 1.0000000e-06, 1.0000000e-06],\n",
       "        [9.9999702e-01, 1.0000000e-06, 1.0000000e-06, 1.0000000e-06]],\n",
       "\n",
       "       [[9.9999702e-01, 1.0000000e-06, 1.0000000e-06, 1.0000000e-06],\n",
       "        [8.3226544e-01, 1.4840166e-01, 1.0235688e-02, 9.0971785e-03],\n",
       "        [9.0341359e-01, 3.7765019e-02, 2.2309702e-02, 3.6511686e-02],\n",
       "        [9.9999702e-01, 1.0000000e-06, 1.0000000e-06, 1.0000000e-06]],\n",
       "\n",
       "       [[9.9999702e-01, 1.0000000e-06, 1.0000000e-06, 1.0000000e-06],\n",
       "        [9.2575294e-01, 3.8013570e-02, 2.7366873e-02, 8.8666305e-03],\n",
       "        [1.7959351e-02, 1.3601107e-02, 9.5569611e-01, 1.2743457e-02],\n",
       "        [1.0000000e-06, 1.0000000e-06, 9.9999702e-01, 1.0000000e-06]],\n",
       "\n",
       "       [[9.9999702e-01, 1.0000000e-06, 1.0000000e-06, 1.0000000e-06],\n",
       "        [9.9999702e-01, 1.0000000e-06, 1.0000000e-06, 1.0000000e-06],\n",
       "        [1.0000000e-06, 1.0000000e-06, 1.0000000e-06, 9.9999702e-01],\n",
       "        [2.5344908e-02, 1.2019626e-02, 1.1299516e-02, 9.5133597e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "name": "train_a_model.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
